{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64364a97-0b72-4faf-986e-428e01b03607",
   "metadata": {},
   "source": [
    "# LangChain and LangGraph examples\n",
    "\n",
    "In this guide, we will see how to integrate Literal AI in a LangGraph workflow.\n",
    "\n",
    "- [Necessary imports](#imports)\n",
    "- [Define tools](#define-available-tools)\n",
    "- [Agent logic](#agent-logic)\n",
    "- [Run agent](#run-agent)\n",
    "\n",
    "<a id=\"imports\"></a>\n",
    "## Necessary imports\n",
    "\n",
    "Make sure to define the `LITERAL_API_KEY`, `OPENAI_API_KEY` and `TAVILY_API_KEY` in your `.env`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cba71ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema.runnable.config import RunnableConfig\n",
    "import os\n",
    "from literalai import LiteralClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccf4ba8",
   "metadata": {},
   "source": [
    "# Check environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "664be3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lsk_Q6RcSpjtJnfl16z8H4lcthmwBqffEVdO4cbUk4Y5o\n",
      "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\n",
      "sk-Pp5mufXH5qVjfvaqyjWa5-RzHLxRlbMgU9m8aVJc7ET3BlbkFJo19FHiCFkUQ0rD2U3MQIyluY3P2788Mvrx9xpvLn0A\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "print(os.getenv(\"LITERAL_API_KEY\"))\n",
    "print(os.getenv(\"TAVILY_API_KEY\"))\n",
    "print(os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98b540c-a99c-46e3-86d7-d68f6bf59f80",
   "metadata": {},
   "source": [
    "<a id=\"define-available-tools\"></a>\n",
    "## Define available tools\n",
    "\n",
    "We will use Tavily as a search tool. `tools` is a list of available tools  (here, we only have one tool, the TavilySearchResults tool)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1eb0ac3b-1a27-4a2b-b4fd-db4da8519bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "literal_client = LiteralClient()\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# Define the tool (TavilySearchResults tool to search the web)\n",
    "tool = TavilySearchResults(max_results=2, k=2)\n",
    "tools = [tool]\n",
    "\n",
    "# Define the LLM (chatGPT 4o-mini)\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c21e18-f6e4-4485-92c6-15b3e08f4292",
   "metadata": {},
   "source": [
    "<a id=\"agent-logic\"></a>\n",
    "## Agent logic\n",
    "\n",
    "For the agent logic, we simply repeat the following pattern (max. 5 times):\n",
    "- ask the user question to the LLM, making the tools available\n",
    "- execute tools if LLM asks for it, otherwise return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33683f5e-c818-4e46-8804-4cfcce066880",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding a node to a graph that has already been compiled. This will not be reflected in the compiled graph.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Node `chatbot` already present.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [llm_with_tools\u001b[38;5;241m.\u001b[39minvoke(state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m])]}\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Add the chatbot node to the graph\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mgraph_builder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_node\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchatbot\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchatbot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Add the tool node to the graph\u001b[39;00m\n\u001b[1;32m     10\u001b[0m tool_node \u001b[38;5;241m=\u001b[39m ToolNode(tools\u001b[38;5;241m=\u001b[39m[tool])\n",
      "File \u001b[0;32m~/Documents/CHAINLIT/venv/lib/python3.12/site-packages/langgraph/graph/state.py:309\u001b[0m, in \u001b[0;36mStateGraph.add_node\u001b[0;34m(self, node, action, metadata, input, retry)\u001b[0m\n\u001b[1;32m    307\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(action, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m, action\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes:\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNode `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` already present.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m node \u001b[38;5;241m==\u001b[39m END \u001b[38;5;129;01mor\u001b[39;00m node \u001b[38;5;241m==\u001b[39m START:\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNode `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` is reserved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Node `chatbot` already present."
     ]
    }
   ],
   "source": [
    "# Define the chatbot logic\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "# Add the chatbot node to the graph\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "# Add the tool node to the graph\n",
    "tool_node = ToolNode(tools=[tool])\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,\n",
    ")\n",
    "# Any time a tool is called, we return to the chatbot to decide the next step\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "graph = graph_builder.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b8b912-059c-4f61-b988-ff8e18ba0363",
   "metadata": {},
   "source": [
    "<a id=\"run-agent\"></a>\n",
    "## Run agent against a question\n",
    "\n",
    "The agent has a pre-set user question (What is the weather in Paris?). It is run in a separate thread to log the output in literal, and then prints the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fc5cd47-8a35-450b-a150-bb027fa3bcf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current weather in Paris is as follows:\n",
      "\n",
      "- **Temperature**: 13.2°C (55.8°F)\n",
      "- **Condition**: Overcast\n",
      "- **Wind**: Northeast at 13.2 mph (21.2 kph)\n",
      "- **Humidity**: 94%\n",
      "- **Visibility**: 10 km\n",
      "- **Pressure**: 1026 mb\n",
      "\n",
      "You can find more detailed weather information [here](https://www.weatherapi.com/). \n",
      "\n",
      "As for the upcoming days, Paris is expected to experience cooler weather with temperatures gradually decreasing as September progresses, averaging around 15°C by the end of the month, along with light to moderate rainfall.\n"
     ]
    }
   ],
   "source": [
    "# wait for user input and then run the graph\n",
    "with literal_client.thread(name=\"Weather in Paris\") as thread:\n",
    "    user_input = \"What is the weather in Paris?\"\n",
    "    cb = literal_client.langchain_callback()\n",
    "    res = graph.invoke({\"messages\": [HumanMessage(content=user_input)]}, config=RunnableConfig(callbacks=[cb]))\n",
    "    print(res[\"messages\"][-1].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
