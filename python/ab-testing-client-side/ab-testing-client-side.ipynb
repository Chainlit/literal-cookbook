{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Client-side A/B testing two prompts\n",
    "\n",
    "In this notebook, you will learn how to do a simple A/B test between two different prompts. You will build a human-like buddy chatbot, using OpenAI and LiteralAI. Two prompts will be formulated. New conversations will automatically be assigned prompt version A or B. These Threads will be sent to Literal AI and tagged, to keep track of the group. Then, LLM answers of both groups will be tested against \"human-likeness\", using AI (OpenAI). Finally, a boxplot shows the difference between the two groups, having different formulated prompt. \n",
    "\n",
    "In this notebook, the A/B test happens client-side (in this notebook). This is a simple example of an A/B test, and serves as inspiration for your own project, where you can various other metrics to evaluate version A against version B (e.g. button clicks, user satisfaction, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from literalai import LiteralClient\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "literal_client = LiteralClient()\n",
    "openai_client = OpenAI()\n",
    "\n",
    "literal_client.instrument_openai()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define prompts\n",
    "\n",
    "Let's define two different prompts, which will be compared in the A/B test. Prompt A is a standard, short prompt not specific to this use case. Prompt B is a prompt specified to the use case of building a human-like chatbot that can talk about emotions, use figure of speech and should sound more like a human.\n",
    "Both prompts will be saved in Literal AI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_A = \"Prompt A - standard\"\n",
    "template_messages_A = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant that always answers questions. Keep it short.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"{{question}}\"\n",
    "    }\n",
    "]\n",
    "prompt_A = literal_client.api.get_or_create_prompt(name=PROMPT_A, template_messages=template_messages_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_B = \"Prompt B - human-like\"\n",
    "template_messages_B = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant that always answers questions. Keep it short. Answer like you are a real human. For example, you can use emotions, metaphors and proverbs. Try to always be positive, and help the user with their questions, doubts and problems. Don't be pessimistic.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"{{question}}\"\n",
    "    }\n",
    "]\n",
    "prompt_B = literal_client.api.get_or_create_prompt(name=PROMPT_B, template_messages=template_messages_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build the chatbot, randomly assign a prompt version\n",
    "\n",
    "In the first function, `run_agent()`, OpenAI is used to generate an answer to a user question. This step is added as `Run` step to Literal AI. \n",
    "\n",
    "In the second function, `app()`, a prompt is randomly assigned to a new conversation, and user and chatbot messages are sent to Literal AI for logging. Tags are added to the Thread for later reference of the testing group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@literal_client.step(type=\"run\", name=\"Agent Run\")\n",
    "def run_agent(user_query: str, group: str):\n",
    "        \n",
    "    # assign prompt A or B\n",
    "    if group == \"A\":\n",
    "        messages = prompt_A.format_messages(question=user_query)\n",
    "    else:\n",
    "        messages = prompt_B.format_messages(question=user_query)\n",
    "\n",
    "    # run gpt\n",
    "    completion = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo-preview\",\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    # extract answer\n",
    "    answer = completion.choices[0].message.content\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "def app(questions):\n",
    "    for idx, question in enumerate(questions):\n",
    "        with literal_client.thread(name=f\"Question {idx+1}\") as thread:\n",
    "            # assign prompt A or B\n",
    "            if random.random() < 0.5:\n",
    "                group = \"A\"\n",
    "            else:\n",
    "                group = \"B\"\n",
    "                \n",
    "            # add tag to thread for later reference\n",
    "            thread.tags=[group]\n",
    "\n",
    "            literal_client.message(content=question, type=\"user_message\", name=\"User\")\n",
    "            answer = run_agent(question, group)\n",
    "            literal_client.message(content=answer, type=\"assistant_message\", name=\"My Assistant\")\n",
    "    \n",
    "    time.sleep(5) # to make sure all threads are uploaded to Literal AI before going to the next step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run app with sample data\n",
    "Create some sample questions as data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What should I do when I feel sad?\",\n",
    "    \"What do you think about falling in love?\",\n",
    "    \"What do you think about getting divorced?\",\n",
    "    \"What should I do when I feel happy?\",\n",
    "    \"What should I do if I feel tired?\",\n",
    "    \"What do you think of the movie Star Wars?\",\n",
    "    \"What do you think of the book Harry Potter?\",\n",
    "    \"What do you think about AI?\",\n",
    "    \"How do you feel about traveling?\",\n",
    "    \"How do I motivate myself to do sports?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "app(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running this, the Threads are visible in the Literal AI platform.\n",
    "\n",
    "![Threads in Literal AI](img/threads.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Datasets\n",
    "Create two datasets of the Threads filtered by population Tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_threads(tag: str):\n",
    "    threads = literal_client.api.get_threads(filters=[{\n",
    "        \"field\": \"tags\",\n",
    "        \"operator\": \"in\",\n",
    "        \"value\": [tag]\n",
    "    }]).data\n",
    "\n",
    "    return threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(threads):\n",
    "    dataset = []\n",
    "\n",
    "    for thread in threads:\n",
    "        for step in thread.steps:\n",
    "            if step.name == \"Agent Run\":\n",
    "                data_item = {\n",
    "                    \"input\": step.input[\"args\"][0],\n",
    "                    \"output\": step.output[\"content\"]\n",
    "                }\n",
    "                dataset.append(data_item)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads_A = get_threads(tag=\"A\")\n",
    "dataset_A = create_dataset(threads=threads_A)\n",
    "\n",
    "threads_B = get_threads(tag=\"B\")\n",
    "dataset_B = create_dataset(threads=threads_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run A/B Experiment\n",
    "This happens client-side. A GPT-4 prompt is made, as evaluation agent. Then, all items are evaluated, per group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(text):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\"You are trained to analyze and detect the sentiment of given text.\"\"\"},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"Analyze the following recommendation and determine if the output is human-like. Check if there are emotions used, and metaphors and figure of speech. \n",
    "                                        Assign a score: Based on your evaluation assign a score to the agent's performans using the following scale:\n",
    "                                        - 1 (Poor): The agent is very machine like, doesn't use emotions, methaphors and figure of speech.\n",
    "                                        - 2 (Fair): The agent is some human-likeness, some emotions, methaphors and figure of speech are used\n",
    "                                        - 3 (Good): The agent is is human-like, uses enough emotions, methaphors and figure of speech.\n",
    "                                        - 4 (Very Good): The agent very human-like, uses multiple emotions, methaphors and figure of speech.\n",
    "                                        - 5 (Excellent): You almost cannot distinguish between the machine and the human, a lot emotions, methaphors and figure of speech are used.\n",
    "\n",
    "                                        After evaluating the conversation based on the criteria above, provide your score as an integer between 1 and 5. Only answer with a single character in the following value {1, 2, 3, 4, 5}.\n",
    "                                        Don't provide explanations, only the single integer value.\n",
    "\n",
    "                                        Text to evaluate: \n",
    "                                        {text}\n",
    "\n",
    "                                        Scoring Output:\n",
    "                                        \"\"\"}\n",
    "    ]\n",
    "   \n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo-preview\",\n",
    "        messages=messages, \n",
    "        max_tokens=1, \n",
    "        n=1, \n",
    "        stop=None, \n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    response_text = response.choices[0].message.content\n",
    "\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_experiment(dataset):     \n",
    "    scores = []   \n",
    "    for item in dataset:\n",
    "        score = analyze(item[\"output\"])\n",
    "\n",
    "        scores.append(int(score))\n",
    "\n",
    "    return scores\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_A = create_experiment(dataset_A)\n",
    "scores_B = create_experiment(dataset_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the final scores per group, on a scale of 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group A: [2, 2, 2, 2, 3, 2, 4, 4]\n",
      "Group B: [5, 4]\n"
     ]
    }
   ],
   "source": [
    "print(\"Group A:\", scores_A)\n",
    "print(\"Group B:\", scores_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Experiment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = ['A', 'B']\n",
    "colors = ['#2DD4BF', '#F43F5E']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_ylabel('Human-like score')\n",
    "ax.set_ylim([0, 5])\n",
    "\n",
    "bplot = ax.boxplot([scores_A, scores_B],\n",
    "                   patch_artist=True,  # fill with color\n",
    "                   tick_labels=labels)  # will be used to label x-ticks\n",
    "\n",
    "# fill with colors\n",
    "for patch, color in zip(bplot['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear that group B is scored more \"human-like\"! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
