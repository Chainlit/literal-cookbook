{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4737836-127c-4d52-badb-810cf9a871c6",
   "metadata": {},
   "source": [
    "# Evaluation of Chainlit-doc Copilot RAG system\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab454dc1",
   "metadata": {},
   "source": [
    "In this notebook, we will evaluate the performance of the Generation step in a RAG system. The following steps are performed:\n",
    "1. Initialize Literal AI SDK\n",
    "2. Create a dataset from Threads in Literal AI\n",
    "3. Evaluate Generation with RAGAS on Answer Relevancy and Faithfulness\n",
    "4. Persist experiment to Literal AI\n",
    "\n",
    "When you evaluate a RAG system, you should not evaluate the Retrieval step and the Generation step. This notebook focusses on evaluating the Generation step only. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023d92be-2c9c-4423-80a5-e9ed303f3528",
   "metadata": {},
   "source": [
    "## 1. Import the Literal AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b8e2e1-7cc7-4673-b91a-1a55f5602aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from literalai import LiteralClient\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "literal_client = LiteralClient(api_key=os.getenv(\"LITERAL_API\"))\n",
    "literal_client.instrument_openai()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05484e7e-708f-43dd-8ed4-6d2460ef747e",
   "metadata": {},
   "source": [
    "## 2. Create a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3158a9c-c9c2-40fc-8554-5eb149f4bf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = f\"RAG-evaluation\"\n",
    "\n",
    "dataset = literal_client.api.get_dataset(name=DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1901bf23-1501-413a-b3ab-dfea64050710",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_threads = 2\n",
    "\n",
    "if not dataset:\n",
    "    dataset = literal_client.api.create_dataset(name=DATASET_NAME)\n",
    "    \n",
    "    threads = literal_client.api.get_threads(first=number_of_threads).data\n",
    "    \n",
    "    rag_steps = []\n",
    "    for thread in threads:\n",
    "        rag_steps.extend([step for step in thread.steps if step.name == \"RAG Agent\"])\n",
    "    \n",
    "    for step in rag_steps:\n",
    "        dataset.add_step(step.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fac5a6c-2a11-407d-ab70-5809dec163a2",
   "metadata": {},
   "source": [
    "## 3. Evaluate with Ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ede3ee-ef76-48b7-9512-b3250f0bc610",
   "metadata": {},
   "source": [
    "#### Prepare Ragas data samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a25e08-3e9a-4466-82ca-e598743df1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "items = dataset.items\n",
    "\n",
    "# Get the retrieved contexts for each question.\n",
    "contexts = []\n",
    "message_histories = []\n",
    "\n",
    "for item in items:\n",
    "    context = []\n",
    "    message_history = item.intermediary_steps\n",
    "\n",
    "    for step in item.intermediary_steps:\n",
    "        if step[\"name\"] == \"Cookbooks Retrieval\" or step[\"name\"] == \"Documentation Retrieval\":\n",
    "            context.extend(ast.literal_eval(step[\"output\"][\"content\"])) # convert string to list\n",
    "      \n",
    "    contexts.append(context)\n",
    "    message_histories.append(message_history)\n",
    "\n",
    "# Data samples, in the format expected by Ragas. No ground truth needed since we will evaluate context relevancy.\n",
    "data_samples = {\n",
    "    'question': [item.input[\"content\"][\"args\"][0] for item in items],\n",
    "    'answer': [item.expected_output[\"content\"] if item.expected_output else \"\" for item in items],\n",
    "    'contexts': contexts,\n",
    "    'ground_truth': [\"\"]*len(items),\n",
    "    'messages': message_histories\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcda447e-4478-4a79-be74-3c36c46e29b1",
   "metadata": {},
   "source": [
    "#### Run the evaluation\n",
    "\n",
    "We will evaluate context relevancy which checks how relevant the retrieved contexts are to answer the user's question. \n",
    "\n",
    "The more unneeded details in the contexts, the less relevant (between 0 and 1, 0 being least relevant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6b662b-7126-4a92-9703-45db0d49557c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_relevancy, faithfulness\n",
    "\n",
    "results = evaluate(Dataset.from_dict(data_samples), metrics=[answer_relevancy, faithfulness]).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0886e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54571f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.faithfulness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149b8c87-25c1-4a95-a593-aaa906f8a17f",
   "metadata": {},
   "source": [
    "## 4. Persist experiment to Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce1359a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = literal_client.api.get_prompt(name=\"RAG prompt - Tooled\")\n",
    "\n",
    "experiment = dataset.create_experiment(\n",
    "    name=\"Experiment RAG\",\n",
    "    prompt_id=prompt.id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d02921-65c2-4f79-9ba1-464a08e6bdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Log each experiment result.\n",
    "for index, row in results.iterrows():\n",
    "    scores = [{ \n",
    "        \"name\": answer_relevancy.name,\n",
    "        \"type\": \"AI\",\n",
    "        \"value\": row[answer_relevancy.name] if (row[answer_relevancy.name] >= 0 and row[answer_relevancy.name] <=1) else 0\n",
    "    }, { \n",
    "        \"name\": faithfulness.name,\n",
    "        \"type\": \"AI\",\n",
    "        \"value\": row[faithfulness.name] if (row[faithfulness.name] >= 0 and row[faithfulness.name] <=1) else 0\n",
    "    }]\n",
    "\n",
    "    experiment_item = {\n",
    "        \"datasetItemId\": items[index].id,\n",
    "        \"scores\": scores,\n",
    "        \"input\": { \"question\": row[\"question\"], \"messages\": row[\"messages\"].tolist(), \"retrieval\": row[\"contexts\"].tolist()},\n",
    "        \"output\": { \"output\": row[\"answer\"] }\n",
    "    }\n",
    "    \n",
    "    experiment.log(experiment_item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
