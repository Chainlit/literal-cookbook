{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4737836-127c-4d52-badb-810cf9a871c6",
   "metadata": {},
   "source": [
    "# Evaluation with Ragas \n",
    "\n",
    "This notebook shows you how to validate changes on your RAG application by running evaluations with Ragas and visualizing your experiments in Literal. \n",
    "\n",
    "We first create a dataset from an example RAG application to then evaluate the impact of a retrieval parameter change (# of contexts) on context relevancy (experiments A and B):\n",
    "- [Run a RAG application](#run-rag-app)\n",
    "- [Create a Dataset](#create-dataset)\n",
    "- [Experiment A](#experiment-a)\n",
    "    - Evaluate with Ragas\n",
    "    - Create Literal experiment\n",
    "- [Experiment B](#experiment-b)\n",
    "    - Evaluate with Ragas\n",
    "    - Create Literal experiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40550040-be5b-4572-8469-a3febbe5344d",
   "metadata": {},
   "source": [
    "<a id=\"run-rag-app\"></a>\n",
    "## Run a RAG application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556e5531-0cf2-459f-bca3-b58161e6b063",
   "metadata": {},
   "source": [
    "### Create a Chroma vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2570961b-b58d-43a4-b848-ef51acad468f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "collection = chroma_client.get_or_create_collection(\"Biography\")\n",
    "collection.add(\n",
    "    documents=[\"My name is John.\", \"My job is coding.\", \"My dog's name is Fido. Fido is an expert fetcher.\"],\n",
    "    ids=[\"id1\", \"id2\", \"id3\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023d92be-2c9c-4423-80a5-e9ed303f3528",
   "metadata": {},
   "source": [
    "### Run a small RAG application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cc23807-af1b-4c1c-a939-9aa277e72466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from literalai import LiteralClient\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "literal_client = LiteralClient()\n",
    "literal_client.instrument_openai()\n",
    "\n",
    "PROMPT_NAME = \"RAG prompt\"\n",
    "template_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant that always answers questions. Keep it short, and if available prefer responding with code.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Answer the question based on the context below.\\nContext:\\n{{#context}}\\n{{.}}\\n{{/context}}\\nQuestion:\\n{{question}}\\nAnswer:\"\n",
    "    }\n",
    "]\n",
    "\n",
    "prompt = literal_client.api.create_prompt(name=PROMPT_NAME, template_messages=template_messages)\n",
    "\n",
    "@literal_client.step(type=\"run\", name=\"RAG\")\n",
    "def rag(user_query: str):\n",
    "    with literal_client.step(type=\"retrieval\", name=\"Retrieve\") as step:\n",
    "        step.input = { \"question\": user_query }\n",
    "        results = collection.query(query_texts=[user_query], n_results=2)\n",
    "        step.output = results\n",
    "\n",
    "    with literal_client.step(type=\"llm\", name=\"LLM\") as step:\n",
    "        step.input = { \"contexts\": results[\"documents\"][0] }\n",
    "        messages = prompt.format({\"context\": results[\"documents\"][0], \"question\": user_query})\n",
    "        \n",
    "        completion = openai_client.chat.completions.create(\n",
    "                    model=\"gpt-3.5-turbo\",\n",
    "                    messages=messages,\n",
    "                )\n",
    "        step.output = { \"answer\": completion.choices[0].message.content }\n",
    "\n",
    "    return step.output[\"answer\"]\n",
    "        \n",
    "def main():\n",
    "    questions = [ \"What's my name?\", \"What's my job?\" ]\n",
    "    for idx, question in enumerate(questions):\n",
    "        with literal_client.thread(name=f\"Question {idx+1}\") as thread:\n",
    "            literal_client.message(content=question, type=\"user_message\", name=\"User\")\n",
    "            answer = rag(question)\n",
    "            literal_client.message(content=answer, type=\"assistant_message\", name=\"My Assistant\")\n",
    "\n",
    "main()\n",
    "\n",
    "# Network requests by the SDK are performed asynchronously.\n",
    "# Invoke flush() to guarantee the completion of all requests prior to the process termination.\n",
    "# WARNING: If you run a continuous server, you should not use this method.\n",
    "literal_client.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05484e7e-708f-43dd-8ed4-6d2460ef747e",
   "metadata": {},
   "source": [
    "<a id=\"create-dataset\"></a>\n",
    "## Create a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26d2c58a-7ed8-4af7-9a34-2a798767f1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = f\"Biography Evaluation Dataset\"\n",
    "\n",
    "dataset = literal_client.api.get_dataset(name=DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1901bf23-1501-413a-b3ab-dfea64050710",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not dataset:\n",
    "    dataset = literal_client.api.create_dataset(name=DATASET_NAME)\n",
    "    \n",
    "    threads = literal_client.api.get_threads(first=2).data\n",
    "    \n",
    "    rag_steps = []\n",
    "    for thread in threads:\n",
    "        rag_steps.extend([step for step in thread.steps if step.name == \"RAG\"])\n",
    "    \n",
    "    for step in rag_steps:\n",
    "        dataset.add_step(step.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bcbc7a-d27f-42d8-bb79-cf3efde1c74e",
   "metadata": {},
   "source": [
    "<a id=\"experiment-a\"></a>\n",
    "## Experiment A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fac5a6c-2a11-407d-ab70-5809dec163a2",
   "metadata": {},
   "source": [
    "### Evaluate with Ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ede3ee-ef76-48b7-9512-b3250f0bc610",
   "metadata": {},
   "source": [
    "#### Prepare Ragas data samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9a25e08-3e9a-4466-82ca-e598743df1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from literalai import DatasetItem\n",
    "from typing import List\n",
    "\n",
    "items = dataset.items\n",
    "\n",
    "# Get the retrieved contexts for each question.\n",
    "contexts = []\n",
    "for item in items:\n",
    "    retrieve_step = next(step for step in item.intermediary_steps if step[\"name\"] == \"Retrieve\")\n",
    "    contexts.append(retrieve_step[\"expectedOutput\"][\"documents\"][0])\n",
    "\n",
    "# Data samples, in the format expected by Ragas. No ground truth needed since we will evaluate context relevancy.\n",
    "data_samples = {\n",
    "    'question': [item.input[\"args\"][0] for item in items],\n",
    "    'answer': [item.expected_output[\"content\"] for item in items],\n",
    "    'contexts': contexts,\n",
    "    'ground_truth': [\"\"]* len(items)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcda447e-4478-4a79-be74-3c36c46e29b1",
   "metadata": {},
   "source": [
    "#### Run the evaluation\n",
    "\n",
    "We will evaluate context relevancy which checks how relevant the retrieved contexts are to answer the user's question. \n",
    "\n",
    "The more unneeded details in the contexts, the less relevant (between 0 and 1, 0 being least relevant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e6b662b-7126-4a92-9703-45db0d49557c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████████████████████████████| 2/2 [00:01<00:00,  1.81it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import context_relevancy\n",
    "\n",
    "results = evaluate(Dataset.from_dict(data_samples), metrics=[context_relevancy]).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149b8c87-25c1-4a95-a593-aaa906f8a17f",
   "metadata": {},
   "source": [
    "## Create Literal experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4d02921-65c2-4f79-9ba1-464a08e6bdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = dataset.create_experiment(\n",
    "    name=\"Biography - Experiment A\",\n",
    "    prompt_id=prompt.id,\n",
    "    params=[{ \"type\": context_relevancy.name, \"top_k\": 2 }]\n",
    ")\n",
    "\n",
    "# Log each experiment result.\n",
    "for index, row in results.iterrows():\n",
    "    scores = [{ \n",
    "        \"name\": context_relevancy.name,\n",
    "        \"type\": \"AI\",\n",
    "        \"value\": row[context_relevancy.name]\n",
    "    }]\n",
    "\n",
    "    experiment_item = {\n",
    "        \"datasetItemId\": items[index].id,\n",
    "        \"scores\": scores,\n",
    "        \"input\": { \"question\": row[\"question\"] },\n",
    "        \"output\": { \"contexts\": row[\"contexts\"].tolist() }\n",
    "    }\n",
    "    \n",
    "    experiment.log(experiment_item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a255d62-13e1-4fef-b2b4-440ff71faf90",
   "metadata": {},
   "source": [
    "<a id=\"experiment-b\"></a>\n",
    "## Experiment B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162011d8-0bf6-4a40-b3ed-f75ee27c3449",
   "metadata": {},
   "source": [
    "### Evaluate with Ragas\n",
    "\n",
    "We evaluate with the first context only to see how context relevancy gets impacted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "120c1515-ad90-4fc8-a0b9-3fabb826cf8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████████████████████████████| 2/2 [00:00<00:00,  2.18it/s]\n"
     ]
    }
   ],
   "source": [
    "data_samples[\"contexts\"] = [x[:1] for x in contexts]\n",
    "\n",
    "results = evaluate(Dataset.from_dict(data_samples), metrics=[context_relevancy]).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff1184b-38ad-4af3-af4c-3120da43a201",
   "metadata": {},
   "source": [
    "### Create Literal experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9301895-458b-4bec-94bc-5fbced435ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = dataset.create_experiment(\n",
    "    name=\"Biography - Experiment B\",\n",
    "    prompt_id=prompt.id,\n",
    "    params=[{ \"type\": context_relevancy.name, \"top_k\": 1 }]\n",
    ")\n",
    "\n",
    "# Log each experiment result.\n",
    "for index, row in results.iterrows():\n",
    "    scores = [{ \n",
    "        \"name\": context_relevancy.name,\n",
    "        \"type\": \"AI\",\n",
    "        \"value\": row[context_relevancy.name]\n",
    "    }]\n",
    "\n",
    "    experiment_item = {\n",
    "        \"datasetItemId\": items[index].id,\n",
    "        \"scores\": scores,\n",
    "        \"input\": { \"question\": row[\"question\"] },\n",
    "        \"output\": { \"contexts\": row[\"contexts\"].tolist() }\n",
    "    }\n",
    "    \n",
    "    experiment.log(experiment_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9563f7-e5a7-489a-a9f7-2052244bd7c5",
   "metadata": {},
   "source": [
    "## Visualize from Literal Experiments !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
